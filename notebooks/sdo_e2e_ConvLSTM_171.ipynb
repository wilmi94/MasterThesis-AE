{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDiUOWkEoq/KtUBeIb0He+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilmi94/MasterThesis-AE/blob/main/notebooks/sdo_e2e_ConvLSTM_171.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SDO/AIA-171A End-to-End ConvLSTM Model\n",
        "\n",
        "model is based on:\n",
        "\n",
        "*Predicting Solar Flares Using a Long Short-term Memory Network. Liu, H., Liu, C., Wang, J. T. L., Wang, H., ApJ., 877:121, 2019.*\n",
        "\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "k1LkMRs6jGua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE56rXoci7hc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "try :\n",
        "    import tensorflow as tf\n",
        "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "except Exception as e:\n",
        "    print('turn off loggins is not supported')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(datafile, flare_label, series_len, start_feature, n_features, mask_value):\n",
        "    df = pd.read_csv(datafile)\n",
        "    df_values = df.values\n",
        "    X = []\n",
        "    y = []\n",
        "    tmp = []\n",
        "    for k in range(start_feature, start_feature + n_features):\n",
        "        tmp.append(mask_value)\n",
        "    for idx in range(0, len(df_values)):\n",
        "        each_series_data = []\n",
        "        row = df_values[idx]\n",
        "        label = row[1][0]\n",
        "        if flare_label == 'C' and (label == 'X' or label == 'M'):\n",
        "            label = 'C'\n",
        "        if flare_label == 'C' and label == 'B':\n",
        "            label = 'N'\n",
        "        has_zero_record = False\n",
        "        # if at least one of the 25 physical feature values is missing, then discard it.\n",
        "        if flare_label == 'C':\n",
        "            if float(row[5]) == 0.0:\n",
        "                has_zero_record = True\n",
        "            if float(row[7]) == 0.0:\n",
        "                has_zero_record = True\n",
        "            for k in range(9, 13):\n",
        "                if float(row[k]) == 0.0:\n",
        "                    has_zero_record = True\n",
        "                    break\n",
        "            for k in range(14, 16):\n",
        "                if float(row[k]) == 0.0:\n",
        "                    has_zero_record = True\n",
        "                    break\n",
        "            if float(row[18]) == 0.0:\n",
        "                has_zero_record = True\n",
        "\n",
        "        if has_zero_record is False:\n",
        "            cur_noaa_num = int(row[3])\n",
        "            each_series_data.append(row[start_feature:start_feature + n_features].tolist())\n",
        "            itr_idx = idx - 1\n",
        "            while itr_idx >= 0 and len(each_series_data) < series_len:\n",
        "                prev_row = df_values[itr_idx]\n",
        "                prev_noaa_num = int(prev_row[3])\n",
        "                if prev_noaa_num != cur_noaa_num:\n",
        "                    break\n",
        "                has_zero_record_tmp = False\n",
        "                if flare_label == 'C':\n",
        "                    if float(row[5]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "                    if float(row[7]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "                    for k in range(9, 13):\n",
        "                        if float(row[k]) == 0.0:\n",
        "                            has_zero_record_tmp = True\n",
        "                            break\n",
        "                    for k in range(14, 16):\n",
        "                        if float(row[k]) == 0.0:\n",
        "                            has_zero_record_tmp = True\n",
        "                            break\n",
        "                    if float(row[18]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "\n",
        "                if len(each_series_data) < series_len and has_zero_record_tmp is True:\n",
        "                    each_series_data.insert(0, tmp)\n",
        "\n",
        "                if len(each_series_data) < series_len and has_zero_record_tmp is False:\n",
        "                    each_series_data.insert(0, prev_row[start_feature:start_feature + n_features].tolist())\n",
        "                itr_idx -= 1\n",
        "\n",
        "            while len(each_series_data) > 0 and len(each_series_data) < series_len:\n",
        "                each_series_data.insert(0, tmp)\n",
        "\n",
        "            if len(each_series_data) > 0:\n",
        "                X.append(np.array(each_series_data).reshape(series_len, n_features).tolist())\n",
        "                y.append(label)\n",
        "    X_arr = np.array(X)\n",
        "    y_arr = np.array(y)\n",
        "    print(X_arr.shape)\n",
        "    return X_arr, y_arr\n",
        "\n",
        "\n",
        "def data_transform(data):\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(data)\n",
        "    encoded_Y = encoder.transform(data)\n",
        "    converteddata = np_utils.to_categorical(encoded_Y)\n",
        "    return converteddata\n",
        "\n",
        "\n",
        "def attention_3d_block(hidden_states, series_len):\n",
        "    hidden_size = int(hidden_states.shape[2])\n",
        "    hidden_states_t = Permute((2, 1), name='attention_input_t')(hidden_states)\n",
        "    hidden_states_t = Reshape((hidden_size, series_len), name='attention_input_reshape')(hidden_states_t)\n",
        "    score_first_part = Dense(series_len, use_bias=False, name='attention_score_vec')(hidden_states_t)\n",
        "    score_first_part_t = Permute((2, 1), name='attention_score_vec_t')(score_first_part)\n",
        "    h_t = Lambda(lambda x: x[:, :, -1], output_shape=(hidden_size, 1), name='last_hidden_state')(hidden_states_t)\n",
        "    score = dot([score_first_part_t, h_t], [2, 1], name='attention_score')\n",
        "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "    context_vector = dot([hidden_states_t, attention_weights], [2, 1], name='context_vector')\n",
        "    context_vector = Reshape((hidden_size,))(context_vector)\n",
        "    h_t = Reshape((hidden_size,))(h_t)\n",
        "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
        "    attention_vector = Dense(hidden_size, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
        "    return attention_vector\n",
        "\n",
        "\n",
        "def lstm(nclass, n_features, series_len):\n",
        "    inputs = Input(shape=(series_len, n_features,))\n",
        "    lstm_out = LSTM(10, return_sequences=True, dropout=0.5)(inputs)\n",
        "    attention_mul = attention_3d_block(lstm_out, series_len)\n",
        "    layer1_out = Dense(200, activation='relu')(attention_mul)\n",
        "    layer2_out = Dense(500, activation='relu')(layer1_out)\n",
        "    output = Dense(nclass, activation='softmax', activity_regularizer=regularizers.l2(0.0001))(layer2_out)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    flare_label = sys.argv[1]\n",
        "    train_again = int(sys.argv[2])\n",
        "    filepath = './'\n",
        "    n_features = 0\n",
        "    if flare_label == 'C':\n",
        "        n_features = 14\n",
        "    start_feature = 5\n",
        "    mask_value = 0\n",
        "    series_len = 10\n",
        "    epochs = 7\n",
        "    batch_size = 256\n",
        "    nclass = 2\n",
        "    result_file = './output.csv'\n",
        "\n",
        "    if train_again == 1:\n",
        "        # Train\n",
        "        X_train_data, y_train_data = load_data(datafile=filepath + 'normalized_training.csv',\n",
        "                                               flare_label=flare_label, series_len=series_len,\n",
        "                                               start_feature=start_feature, n_features=n_features,\n",
        "                                               mask_value=mask_value)\n",
        "\n",
        "        X_train = np.array(X_train_data)\n",
        "        y_train = np.array(y_train_data)\n",
        "        y_train_tr = data_transform(y_train)\n",
        "\n",
        "        class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                          np.unique(y_train), y_train)\n",
        "        class_weight_ = {0: class_weights[0], 1: class_weights[1]}\n",
        "        # print(class_weight_)\n",
        "\n",
        "        model = lstm(nclass, n_features, series_len)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(X_train, y_train_tr,\n",
        "                            epochs=epochs, batch_size=batch_size,\n",
        "                            verbose=False, shuffle=True, class_weight=class_weight_)\n",
        "        model.save('./model.h5')\n",
        "    else:\n",
        "        model = load_model('./model.h5')\n",
        "\n",
        "    # Test\n",
        "    X_test_data, y_test_data = load_data(datafile=filepath + 'normalized_testing.csv',\n",
        "                                         flare_label=flare_label, series_len=series_len,\n",
        "                                         start_feature=start_feature, n_features=n_features,\n",
        "                                         mask_value=mask_value)\n",
        "    X_test = np.array(X_test_data)\n",
        "    y_test = np.array(y_test_data)\n",
        "    y_test_tr = data_transform(y_test)\n",
        "\n",
        "    classes = model.predict(X_test, batch_size=batch_size, verbose=0, steps=None)\n",
        "\n",
        "    with open(result_file, 'w', encoding='UTF-8') as result_csv:\n",
        "        w = csv.writer(result_csv)\n",
        "        with open(filepath + 'normalized_testing.csv', encoding='UTF-8') as data_csv:\n",
        "            reader = csv.reader(data_csv)\n",
        "            i = -1\n",
        "            for line in reader:\n",
        "                if i == -1:\n",
        "                    line.insert(0, 'Predicted Label')\n",
        "                else:\n",
        "                    if classes[i][0] >= 0.5:\n",
        "                        line.insert(0, 'Positive')\n",
        "                    else:\n",
        "                        line.insert(0, 'Negative')\n",
        "                i += 1\n",
        "                w.writerow(line)"
      ],
      "metadata": {
        "id": "huxvz35GjpfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## from C\n"
      ],
      "metadata": {
        "id": "Sxa7ZahbooYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img = cv2.imread(os.path.join(folder_path, filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return images\n",
        "\n",
        "def create_dataset(data_folder, sequence_length, target_time_steps):\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    for event_folder in os.listdir(data_folder):\n",
        "        event_path = os.path.join(data_folder, event_folder)\n",
        "        event_images = load_images_from_folder(event_path)\n",
        "\n",
        "        # Create sequences for the event images\n",
        "        event_sequences = create_sequences(event_images, sequence_length)\n",
        "\n",
        "        # Create labels for the event sequences\n",
        "        event_labels = create_labels(event_sequences, target_time_steps)\n",
        "\n",
        "        # Append the event sequences and labels to the main dataset\n",
        "        dataset.extend(event_sequences)\n",
        "        labels.extend(event_labels)\n",
        "\n",
        "    return np.array(dataset), np.array(labels)\n",
        "\n",
        "# Function to create sequences of AIA images with a fixed time duration\n",
        "def create_sequences(images, sequence_length):\n",
        "    sequences = []\n",
        "    num_images = len(images)\n",
        "    for i in range(0, num_images - sequence_length + 1, sequence_length):\n",
        "        sequence = images[i:i + sequence_length]\n",
        "        sequences.append(sequence)\n",
        "    return sequences\n",
        "\n",
        "# Function to create target labels for sequences\n",
        "def create_labels(sequences, target_time_steps):\n",
        "    labels = []\n",
        "    for sequence in sequences:\n",
        "        # Check if a CME occurs within the next target_time_steps\n",
        "        cme_occurs = any(sequence[i]['CME_occurs'] for i in range(target_time_steps))\n",
        "        labels.append(int(cme_occurs))\n",
        "    return labels\n",
        "\n",
        "# Assuming the AIA image data is organized in separate event folders within the 'data_folder'\n",
        "data_folder = '/path/to/data_folder'\n",
        "sequence_length = 6  # Choose the number of images in each sequence\n",
        "target_time_steps = 12  # Choose the time steps (intervals) for predicting CME occurrence\n",
        "\n",
        "# Create the dataset and labels\n",
        "dataset, labels = create_dataset(data_folder, sequence_length, target_time_steps)\n"
      ],
      "metadata": {
        "id": "82s_0IYboq7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, ConvLSTM2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Assuming you have already created the 'dataset' and 'labels' using the provided code\n",
        "\n",
        "# Normalize the pixel values in the dataset to a range [0, 1]\n",
        "dataset = dataset.astype('float32') / 255.0\n",
        "\n",
        "# Reshape the dataset to match the input shape expected by ConvLSTM\n",
        "# Assuming the images have shape (image_height, image_width, num_channels)\n",
        "# and 'sequence_length' images per sequence\n",
        "sequence_length, image_height, image_width, num_channels = dataset.shape\n",
        "input_shape = (sequence_length, image_height, image_width, num_channels)\n",
        "\n",
        "# Build the ConvLSTM model\n",
        "model = Sequential()\n",
        "model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_samples = int(0.8 * len(dataset))  # 80% for training\n",
        "x_train, y_train = dataset[:train_samples], labels[:train_samples]\n",
        "x_test, y_test = dataset[train_samples:], labels[train_samples:]\n",
        "\n",
        "# Train the ConvLSTM model\n",
        "epochs = 10  # Adjust the number of epochs based on your dataset and model complexity\n",
        "batch_size = 32\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "VmRWaNccoxOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}