{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOINjinnsN9tzywNMFi1fsI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilmi94/MasterThesis-AE/blob/main/notebooks/sdo_e2e_ConvLSTM_171.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SDO/AIA-171A End-to-End ConvLSTM Model\n",
        "\n",
        "model is based on:\n",
        "\n",
        "*Predicting Solar Flares Using a Long Short-term Memory Network. Liu, H., Liu, C., Wang, J. T. L., Wang, H., ApJ., 877:121, 2019.*\n"
      ],
      "metadata": {
        "id": "k1LkMRs6jGua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE56rXoci7hc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "try :\n",
        "    import tensorflow as tf\n",
        "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "except Exception as e:\n",
        "    print('turn off loggins is not supported')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(datafile, flare_label, series_len, start_feature, n_features, mask_value):\n",
        "    df = pd.read_csv(datafile)\n",
        "    df_values = df.values\n",
        "    X = []\n",
        "    y = []\n",
        "    tmp = []\n",
        "    for k in range(start_feature, start_feature + n_features):\n",
        "        tmp.append(mask_value)\n",
        "    for idx in range(0, len(df_values)):\n",
        "        each_series_data = []\n",
        "        row = df_values[idx]\n",
        "        label = row[1][0]\n",
        "        if flare_label == 'C' and (label == 'X' or label == 'M'):\n",
        "            label = 'C'\n",
        "        if flare_label == 'C' and label == 'B':\n",
        "            label = 'N'\n",
        "        has_zero_record = False\n",
        "        # if at least one of the 25 physical feature values is missing, then discard it.\n",
        "        if flare_label == 'C':\n",
        "            if float(row[5]) == 0.0:\n",
        "                has_zero_record = True\n",
        "            if float(row[7]) == 0.0:\n",
        "                has_zero_record = True\n",
        "            for k in range(9, 13):\n",
        "                if float(row[k]) == 0.0:\n",
        "                    has_zero_record = True\n",
        "                    break\n",
        "            for k in range(14, 16):\n",
        "                if float(row[k]) == 0.0:\n",
        "                    has_zero_record = True\n",
        "                    break\n",
        "            if float(row[18]) == 0.0:\n",
        "                has_zero_record = True\n",
        "\n",
        "        if has_zero_record is False:\n",
        "            cur_noaa_num = int(row[3])\n",
        "            each_series_data.append(row[start_feature:start_feature + n_features].tolist())\n",
        "            itr_idx = idx - 1\n",
        "            while itr_idx >= 0 and len(each_series_data) < series_len:\n",
        "                prev_row = df_values[itr_idx]\n",
        "                prev_noaa_num = int(prev_row[3])\n",
        "                if prev_noaa_num != cur_noaa_num:\n",
        "                    break\n",
        "                has_zero_record_tmp = False\n",
        "                if flare_label == 'C':\n",
        "                    if float(row[5]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "                    if float(row[7]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "                    for k in range(9, 13):\n",
        "                        if float(row[k]) == 0.0:\n",
        "                            has_zero_record_tmp = True\n",
        "                            break\n",
        "                    for k in range(14, 16):\n",
        "                        if float(row[k]) == 0.0:\n",
        "                            has_zero_record_tmp = True\n",
        "                            break\n",
        "                    if float(row[18]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "\n",
        "                if len(each_series_data) < series_len and has_zero_record_tmp is True:\n",
        "                    each_series_data.insert(0, tmp)\n",
        "\n",
        "                if len(each_series_data) < series_len and has_zero_record_tmp is False:\n",
        "                    each_series_data.insert(0, prev_row[start_feature:start_feature + n_features].tolist())\n",
        "                itr_idx -= 1\n",
        "\n",
        "            while len(each_series_data) > 0 and len(each_series_data) < series_len:\n",
        "                each_series_data.insert(0, tmp)\n",
        "\n",
        "            if len(each_series_data) > 0:\n",
        "                X.append(np.array(each_series_data).reshape(series_len, n_features).tolist())\n",
        "                y.append(label)\n",
        "    X_arr = np.array(X)\n",
        "    y_arr = np.array(y)\n",
        "    print(X_arr.shape)\n",
        "    return X_arr, y_arr\n",
        "\n",
        "\n",
        "def data_transform(data):\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(data)\n",
        "    encoded_Y = encoder.transform(data)\n",
        "    converteddata = np_utils.to_categorical(encoded_Y)\n",
        "    return converteddata\n",
        "\n",
        "\n",
        "def attention_3d_block(hidden_states, series_len):\n",
        "    hidden_size = int(hidden_states.shape[2])\n",
        "    hidden_states_t = Permute((2, 1), name='attention_input_t')(hidden_states)\n",
        "    hidden_states_t = Reshape((hidden_size, series_len), name='attention_input_reshape')(hidden_states_t)\n",
        "    score_first_part = Dense(series_len, use_bias=False, name='attention_score_vec')(hidden_states_t)\n",
        "    score_first_part_t = Permute((2, 1), name='attention_score_vec_t')(score_first_part)\n",
        "    h_t = Lambda(lambda x: x[:, :, -1], output_shape=(hidden_size, 1), name='last_hidden_state')(hidden_states_t)\n",
        "    score = dot([score_first_part_t, h_t], [2, 1], name='attention_score')\n",
        "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "    context_vector = dot([hidden_states_t, attention_weights], [2, 1], name='context_vector')\n",
        "    context_vector = Reshape((hidden_size,))(context_vector)\n",
        "    h_t = Reshape((hidden_size,))(h_t)\n",
        "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
        "    attention_vector = Dense(hidden_size, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
        "    return attention_vector\n",
        "\n",
        "\n",
        "def lstm(nclass, n_features, series_len):\n",
        "    inputs = Input(shape=(series_len, n_features,))\n",
        "    lstm_out = LSTM(10, return_sequences=True, dropout=0.5)(inputs)\n",
        "    attention_mul = attention_3d_block(lstm_out, series_len)\n",
        "    layer1_out = Dense(200, activation='relu')(attention_mul)\n",
        "    layer2_out = Dense(500, activation='relu')(layer1_out)\n",
        "    output = Dense(nclass, activation='softmax', activity_regularizer=regularizers.l2(0.0001))(layer2_out)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    flare_label = sys.argv[1]\n",
        "    train_again = int(sys.argv[2])\n",
        "    filepath = './'\n",
        "    n_features = 0\n",
        "    if flare_label == 'C':\n",
        "        n_features = 14\n",
        "    start_feature = 5\n",
        "    mask_value = 0\n",
        "    series_len = 10\n",
        "    epochs = 7\n",
        "    batch_size = 256\n",
        "    nclass = 2\n",
        "    result_file = './output.csv'\n",
        "\n",
        "    if train_again == 1:\n",
        "        # Train\n",
        "        X_train_data, y_train_data = load_data(datafile=filepath + 'normalized_training.csv',\n",
        "                                               flare_label=flare_label, series_len=series_len,\n",
        "                                               start_feature=start_feature, n_features=n_features,\n",
        "                                               mask_value=mask_value)\n",
        "\n",
        "        X_train = np.array(X_train_data)\n",
        "        y_train = np.array(y_train_data)\n",
        "        y_train_tr = data_transform(y_train)\n",
        "\n",
        "        class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                          np.unique(y_train), y_train)\n",
        "        class_weight_ = {0: class_weights[0], 1: class_weights[1]}\n",
        "        # print(class_weight_)\n",
        "\n",
        "        model = lstm(nclass, n_features, series_len)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(X_train, y_train_tr,\n",
        "                            epochs=epochs, batch_size=batch_size,\n",
        "                            verbose=False, shuffle=True, class_weight=class_weight_)\n",
        "        model.save('./model.h5')\n",
        "    else:\n",
        "        model = load_model('./model.h5')\n",
        "\n",
        "    # Test\n",
        "    X_test_data, y_test_data = load_data(datafile=filepath + 'normalized_testing.csv',\n",
        "                                         flare_label=flare_label, series_len=series_len,\n",
        "                                         start_feature=start_feature, n_features=n_features,\n",
        "                                         mask_value=mask_value)\n",
        "    X_test = np.array(X_test_data)\n",
        "    y_test = np.array(y_test_data)\n",
        "    y_test_tr = data_transform(y_test)\n",
        "\n",
        "    classes = model.predict(X_test, batch_size=batch_size, verbose=0, steps=None)\n",
        "\n",
        "    with open(result_file, 'w', encoding='UTF-8') as result_csv:\n",
        "        w = csv.writer(result_csv)\n",
        "        with open(filepath + 'normalized_testing.csv', encoding='UTF-8') as data_csv:\n",
        "            reader = csv.reader(data_csv)\n",
        "            i = -1\n",
        "            for line in reader:\n",
        "                if i == -1:\n",
        "                    line.insert(0, 'Predicted Label')\n",
        "                else:\n",
        "                    if classes[i][0] >= 0.5:\n",
        "                        line.insert(0, 'Positive')\n",
        "                    else:\n",
        "                        line.insert(0, 'Negative')\n",
        "                i += 1\n",
        "                w.writerow(line)"
      ],
      "metadata": {
        "id": "huxvz35GjpfY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}