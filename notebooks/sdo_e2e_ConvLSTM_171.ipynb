{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOezAbVACx3FeOqYDW+wzQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilmi94/MasterThesis-AE/blob/main/notebooks/sdo_e2e_ConvLSTM_171.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SDO/AIA-171A End-to-End ConvLSTM Model\n",
        "\n",
        "> This notebook is part of the Master Thesis *Predicting Coronal Mass Ejections using Machine Learning methods* by Wilmar Ender, FH Wiener Neustadt, 2023.\n",
        "\n",
        "**Objective:** \\\\\n",
        "This notebook aims to perform simple data exploration tasks on  SDO/AIA dataset.\n",
        "\n",
        "**Solar event list:** \\\\\n",
        "*Liu et al. 2020, Predicting Coronal Mass Ejections Using SDO/HMI Vector Magnetic Data Products and Recurrent Neural Networks*\n",
        "\n",
        "**Dataset:** \\\\\n",
        "*Ahmadzadeh et al. 2019, A Curated Image Parameter Data Set from the Solar Dynamics Observatory Mission*. \\\\\n",
        "Accessed via *sdo-cli* (https://github.com/i4Ds/sdo-cli)\n",
        "\n",
        "*Predicting Solar Flares Using a Long Short-term Memory Network. Liu, H., Liu, C., Wang, J. T. L., Wang, H., ApJ., 877:121, 2019.*\n",
        "\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "k1LkMRs6jGua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Notebook"
      ],
      "metadata": {
        "id": "d4uFh-xsFi6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install -U sdo-cli"
      ],
      "metadata": {
        "id": "XhBQXF_EHXts"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yE56rXoci7hc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "try :\n",
        "    import tensorflow as tf\n",
        "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "except Exception as e:\n",
        "    print('turn off loggins is not supported')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70aQqsAbF0LQ",
        "outputId": "cd59a604-08c3-4e02-85da-999d69005d94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change present working directory\n",
        "%cd /content/drive/MyDrive/Academia/MSc. Aerospace Engineering - FH Wiener Neustadt/4. Master Thesis/03-Work/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CWiZE0WF2dm",
        "outputId": "5f8a9c87-2866-42f9-e3ab-3b9f70473544"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Academia/MSc. Aerospace Engineering - FH Wiener Neustadt/4. Master Thesis/03-Work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check content\n",
        "!ls -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEJVDT7mF8IY",
        "outputId": "816023c9-95a3-4b38-eeb2-c07d8c6b5d45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 00_Dataset\t\t\t\t    save_at_14.keras   save_at_24.keras\n",
            " 01_sdo_data_exploration\t\t    save_at_15.keras   save_at_25.keras\n",
            " 02_sdo_binclass\t\t\t    save_at_16.keras   save_at_2.keras\n",
            " 03_sdo_ConvLSTM\t\t\t    save_at_17.keras   save_at_3.keras\n",
            " 04_Tests\t\t\t\t    save_at_18.keras   save_at_4.keras\n",
            "'Master Thesis-ML-Project-Checklist.gdoc'   save_at_19.keras   save_at_5.keras\n",
            " model.png\t\t\t\t    save_at_1.keras    save_at_6.keras\n",
            " save_at_10.keras\t\t\t    save_at_20.keras   save_at_7.keras\n",
            " save_at_11.keras\t\t\t    save_at_21.keras   save_at_8.keras\n",
            " save_at_12.keras\t\t\t    save_at_22.keras   save_at_9.keras\n",
            " save_at_13.keras\t\t\t    save_at_23.keras   .sdo-cli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "Dsou8c2gFkRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_filenames_with_dataframe(directory, dataframe, wavelength):\n",
        "    file_end = '_' + str(wavelength) + '.jpeg'\n",
        "    # Get list of filenames from the directory\n",
        "    directory_filenames = [filename for filename in os.listdir(directory) if filename.endswith(file_end)]\n",
        "\n",
        "    # Get list of names from the DataFrame\n",
        "    df_check = dataframe.copy()\n",
        "    df_check['Timestamp'] = pd.to_datetime(df_check['Timestamp']).dt.strftime('%Y-%m-%dT%H%M%S__171.jpeg') # Warning! wavelength is now hard coded\n",
        "\n",
        "    dataframe_names = df_check['Timestamp'].tolist()  # the name of the image should correspond to the timestamp\n",
        "\n",
        "    # Compare filenames\n",
        "    common_filenames = set(directory_filenames) & set(dataframe_names)\n",
        "    missing_filenames = set(dataframe_names) - set(directory_filenames)\n",
        "    extra_filenames = set(directory_filenames) - set(dataframe_names)\n",
        "\n",
        "    df_missing = pd.DataFrame (data = missing_filenames,  columns=['Timestamp'])\n",
        "    df_missing['Timestamp'] = df_missing['Timestamp'].str.replace(r'__171.jpeg', '', regex=True)\n",
        "    df_missing['Timestamp'] = pd.to_datetime(df_missing['Timestamp']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_directory_files = len(directory_filenames)\n",
        "    total_dataframe_names = len(dataframe_names)\n",
        "    total_common_files = len(common_filenames)\n",
        "    total_missing_files = len(missing_filenames)\n",
        "    total_extra_files = len(extra_filenames)\n",
        "    print('Total Directory Files: ', total_directory_files)\n",
        "    print('Total DataFrame Names: ', total_dataframe_names)\n",
        "    print('Common Files: ', total_common_files)\n",
        "    print('Missing Files: ', total_missing_files)\n",
        "    print('Extra Files: ', total_extra_files)\n",
        "\n",
        "    statistics = {\n",
        "        'Total Directory Files': total_directory_files,\n",
        "        'Total DataFrame Names': total_dataframe_names,\n",
        "        'Common Files': total_common_files,\n",
        "        'Missing Files': total_missing_files,\n",
        "        'Extra Files': total_extra_files,\n",
        "        #'Common File Names': common_filenames,\n",
        "        'Missing File Names': missing_filenames,\n",
        "        'Extra File Names': extra_filenames\n",
        "    }\n",
        "\n",
        "    return statistics, df_missing"
      ],
      "metadata": {
        "id": "pVzyxgFiSDz-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(datafile, flare_label, series_len, start_feature, n_features, mask_value):\n",
        "    df = pd.read_csv(datafile)\n",
        "    df_values = df.values\n",
        "    X = []\n",
        "    y = []\n",
        "    tmp = []\n",
        "    for k in range(start_feature, start_feature + n_features):\n",
        "        tmp.append(mask_value)\n",
        "    for idx in range(0, len(df_values)):\n",
        "        each_series_data = []\n",
        "        row = df_values[idx]\n",
        "        label = row[1][0]\n",
        "        if flare_label == 'C' and (label == 'X' or label == 'M'):\n",
        "            label = 'C'\n",
        "        if flare_label == 'C' and label == 'B':\n",
        "            label = 'N'\n",
        "        has_zero_record = False\n",
        "        # if at least one of the 25 physical feature values is missing, then discard it.\n",
        "        if flare_label == 'C':\n",
        "            if float(row[5]) == 0.0:\n",
        "                has_zero_record = True\n",
        "            if float(row[7]) == 0.0:\n",
        "                has_zero_record = True\n",
        "            for k in range(9, 13):\n",
        "                if float(row[k]) == 0.0:\n",
        "                    has_zero_record = True\n",
        "                    break\n",
        "            for k in range(14, 16):\n",
        "                if float(row[k]) == 0.0:\n",
        "                    has_zero_record = True\n",
        "                    break\n",
        "            if float(row[18]) == 0.0:\n",
        "                has_zero_record = True\n",
        "\n",
        "        if has_zero_record is False:\n",
        "            cur_noaa_num = int(row[3])\n",
        "            each_series_data.append(row[start_feature:start_feature + n_features].tolist())\n",
        "            itr_idx = idx - 1\n",
        "            while itr_idx >= 0 and len(each_series_data) < series_len:\n",
        "                prev_row = df_values[itr_idx]\n",
        "                prev_noaa_num = int(prev_row[3])\n",
        "                if prev_noaa_num != cur_noaa_num:\n",
        "                    break\n",
        "                has_zero_record_tmp = False\n",
        "                if flare_label == 'C':\n",
        "                    if float(row[5]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "                    if float(row[7]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "                    for k in range(9, 13):\n",
        "                        if float(row[k]) == 0.0:\n",
        "                            has_zero_record_tmp = True\n",
        "                            break\n",
        "                    for k in range(14, 16):\n",
        "                        if float(row[k]) == 0.0:\n",
        "                            has_zero_record_tmp = True\n",
        "                            break\n",
        "                    if float(row[18]) == 0.0:\n",
        "                        has_zero_record_tmp = True\n",
        "\n",
        "                if len(each_series_data) < series_len and has_zero_record_tmp is True:\n",
        "                    each_series_data.insert(0, tmp)\n",
        "\n",
        "                if len(each_series_data) < series_len and has_zero_record_tmp is False:\n",
        "                    each_series_data.insert(0, prev_row[start_feature:start_feature + n_features].tolist())\n",
        "                itr_idx -= 1\n",
        "\n",
        "            while len(each_series_data) > 0 and len(each_series_data) < series_len:\n",
        "                each_series_data.insert(0, tmp)\n",
        "\n",
        "            if len(each_series_data) > 0:\n",
        "                X.append(np.array(each_series_data).reshape(series_len, n_features).tolist())\n",
        "                y.append(label)\n",
        "    X_arr = np.array(X)\n",
        "    y_arr = np.array(y)\n",
        "    print(X_arr.shape)\n",
        "    return X_arr, y_arr\n",
        "\n",
        "\n",
        "def data_transform(data):\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(data)\n",
        "    encoded_Y = encoder.transform(data)\n",
        "    converteddata = np_utils.to_categorical(encoded_Y)\n",
        "    return converteddata\n",
        "\n",
        "\n",
        "def attention_3d_block(hidden_states, series_len):\n",
        "    hidden_size = int(hidden_states.shape[2])\n",
        "    hidden_states_t = Permute((2, 1), name='attention_input_t')(hidden_states)\n",
        "    hidden_states_t = Reshape((hidden_size, series_len), name='attention_input_reshape')(hidden_states_t)\n",
        "    score_first_part = Dense(series_len, use_bias=False, name='attention_score_vec')(hidden_states_t)\n",
        "    score_first_part_t = Permute((2, 1), name='attention_score_vec_t')(score_first_part)\n",
        "    h_t = Lambda(lambda x: x[:, :, -1], output_shape=(hidden_size, 1), name='last_hidden_state')(hidden_states_t)\n",
        "    score = dot([score_first_part_t, h_t], [2, 1], name='attention_score')\n",
        "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "    context_vector = dot([hidden_states_t, attention_weights], [2, 1], name='context_vector')\n",
        "    context_vector = Reshape((hidden_size,))(context_vector)\n",
        "    h_t = Reshape((hidden_size,))(h_t)\n",
        "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
        "    attention_vector = Dense(hidden_size, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
        "    return attention_vector\n",
        "\n",
        "\n",
        "def lstm(nclass, n_features, series_len):\n",
        "    inputs = Input(shape=(series_len, n_features,))\n",
        "    lstm_out = LSTM(10, return_sequences=True, dropout=0.5)(inputs)\n",
        "    attention_mul = attention_3d_block(lstm_out, series_len)\n",
        "    layer1_out = Dense(200, activation='relu')(attention_mul)\n",
        "    layer2_out = Dense(500, activation='relu')(layer1_out)\n",
        "    output = Dense(nclass, activation='softmax', activity_regularizer=regularizers.l2(0.0001))(layer2_out)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "huxvz35GjpfY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try 1: DS creation from Chat GPT\n"
      ],
      "metadata": {
        "id": "Sxa7ZahbooYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img = cv2.imread(os.path.join(folder_path, filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return images\n",
        "\n",
        "def create_dataset(data_folder, sequence_length, target_time_steps):\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    for event_folder in os.listdir(data_folder):\n",
        "        event_path = os.path.join(data_folder, event_folder)\n",
        "        event_images = load_images_from_folder(event_path)\n",
        "\n",
        "        # Create sequences for the event images\n",
        "        event_sequences = create_sequences(event_images, sequence_length)\n",
        "\n",
        "        # Create labels for the event sequences\n",
        "        event_labels = create_labels(event_sequences, target_time_steps)\n",
        "\n",
        "        # Append the event sequences and labels to the main dataset\n",
        "        dataset.extend(event_sequences)\n",
        "        labels.extend(event_labels)\n",
        "\n",
        "    return np.array(dataset), np.array(labels)\n",
        "\n",
        "# Function to create sequences of AIA images with a fixed time duration\n",
        "def create_sequences(images, sequence_length):\n",
        "    sequences = []\n",
        "    num_images = len(images)\n",
        "    for i in range(0, num_images - sequence_length + 1, sequence_length):\n",
        "        sequence = images[i:i + sequence_length]\n",
        "        sequences.append(sequence)\n",
        "    return sequences\n",
        "\n",
        "# Function to create target labels for sequences\n",
        "def create_labels(sequences, target_time_steps):\n",
        "    labels = []\n",
        "    for sequence in sequences:\n",
        "        # Check if a CME occurs within the next target_time_steps\n",
        "        cme_occurs = any(sequence[i]['CME_occurs'] for i in range(target_time_steps))\n",
        "        labels.append(int(cme_occurs))\n",
        "    return labels\n",
        "\n",
        "# Assuming the AIA image data is organized in separate event folders within the 'data_folder'\n",
        "data_folder = '/path/to/data_folder'\n",
        "sequence_length = 6  # Choose the number of images in each sequence\n",
        "target_time_steps = 12  # Choose the time steps (intervals) for predicting CME occurrence\n",
        "\n",
        "# Create the dataset and labels\n",
        "dataset, labels = create_dataset(data_folder, sequence_length, target_time_steps)\n"
      ],
      "metadata": {
        "id": "82s_0IYboq7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "e7458707-5c0c-4e1a-81dd-ad307f5164ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ef73eba190e9>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Create the dataset and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_time_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ef73eba190e9>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(data_folder, sequence_length, target_time_steps)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mevent_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mevent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mevent_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/data_folder'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try 2: DS creation from Chat GPT"
      ],
      "metadata": {
        "id": "uu4OyqsBP2LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_directory(directory):\n",
        "    images = []\n",
        "    timestamps = []\n",
        "    for filename in sorted(os.listdir(directory)):\n",
        "        if filename.endswith(\".jpeg\"):\n",
        "            img = load_img(os.path.join(directory, filename))\n",
        "            img_array = img_to_array(img)\n",
        "            images.append(img_array)\n",
        "            timestamp = filename.split(\"__\")[0]\n",
        "            timestamps.append(timestamp)\n",
        "    return images, timestamps\n",
        "\n",
        "def create_conv_lstm_dataset(data_dir, T, test_size=0.2):\n",
        "    pos_dir = os.path.join(data_dir, 'pos')\n",
        "    neg_dir = os.path.join(data_dir, 'neg')\n",
        "\n",
        "    pos_images, pos_timestamps = load_images_from_directory(pos_dir)\n",
        "    neg_images, neg_timestamps = load_images_from_directory(neg_dir)\n",
        "\n",
        "    all_images = np.array(pos_images + neg_images)\n",
        "    all_timestamps = pos_timestamps + neg_timestamps\n",
        "    labels = np.array([1] * len(pos_images) + [0] * len(neg_images))\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(all_images) - T):\n",
        "        sequence = all_images[i:i + T]\n",
        "        label = labels[i + T]\n",
        "        X.append(sequence)\n",
        "        y.append(label)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test, timestamps_train, timestamps_test = train_test_split(\n",
        "        X, y, all_timestamps[T:], test_size=test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, timestamps_train, timestamps_test\n",
        "\n",
        "data_dir = '/data'\n",
        "T = 5  # Number of time steps for the ConvLSTM sequence\n",
        "X_train, y_train, X_test, y_test, timestamps_train, timestamps_test = create_conv_lstm_dataset(data_dir, T)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "# Define your ConvLSTM model using Keras here\n",
        "# ...\n",
        "\n",
        "# Compile and train the model\n",
        "# model.compile(...)\n",
        "# model.fit(...)\n"
      ],
      "metadata": {
        "id": "NH-_lSuTP1ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Dataset"
      ],
      "metadata": {
        "id": "36wobOsHFtSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Event-Lists\n",
        "\n",
        "Same list from *Liu et al. 2020*, however the events were sampled for predicting CMEs within the next x hours (x = 12, 24, 36, 48 or 60).\n",
        "\n",
        "> N means there is a >=M class flare within the next x hours but this flare is not associated with a CME. \\\\\n",
        "> P means there is a >=M class flare within the next x hours and this flare is associated with a CME. \\\\\n",
        "> The second column is titled Timestamp. \\\\\n",
        "> The third column and fourth column are titled NOAA active region number and HARP number, respectively. \\\\\n",
        "> Starting from the fifth column, you can see physical parameters of data samples, which include 18 SHARP parameters:\n",
        "TOTUSJH, TOTPOT, TOTUSJZ, ABSNJZH, SAVNCPP, USFLUX, AREA_ACR, MEANPOT, R_VALUE, SHRGT45, MEANGAM, MEANJZH, MEANGBT, MEANGBZ, MEANJZD, MEANGBH, MEANSHR, MEANALP."
      ],
      "metadata": {
        "id": "owMfqyhRIU6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base Event List"
      ],
      "metadata": {
        "id": "qnNfXbffLLiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load list with both (neg, pos) labels\n",
        "df_cme_list = pd.read_csv(r'00_Dataset/event_lists/all_cme_events.csv', delimiter =';')\n",
        "\n",
        "# convert time stamp such that sdo-cli can read them\n",
        "df_cme_list['Start Time'] = pd.to_datetime(df_cme_list['Start Time']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "df_cme_list['Peak Time'] = pd.to_datetime(df_cme_list['Peak Time']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "df_cme_list['End Time'] = pd.to_datetime(df_cme_list['End Time']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "df_cme_list.head()"
      ],
      "metadata": {
        "id": "eAQ2G5nnGguH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training List"
      ],
      "metadata": {
        "id": "D7d3bIEpLQ1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load list with both (neg, pos) labels of training list for t= 12 h prediction window\n",
        "df_cme_training_12 = pd.read_csv(r'00_Dataset/event_lists/Liu2020_CME_data_samples/normalized_training_12.csv', delimiter =',')\n",
        "df_cme_training_12.shape\n",
        "df_cme_training_12.head()"
      ],
      "metadata": {
        "id": "MaRBWJT7LVM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce dataframe to relevant columns\n",
        "df_cme_training_12 = df_cme_training_12.drop(['TOTUSJH',\t'TOTPOT',\t'TOTUSJZ',\t'ABSNJZH',\t'SAVNCPP',\n",
        "                     'USFLUX', 'AREA_ACR', 'MEANPOT', 'R_VALUE', 'SHRGT45',\n",
        "                     'MEANGAM', 'MEANJZH', 'MEANGBT', 'MEANGBZ', 'MEANJZD',\n",
        "                     'MEANGBH', 'MEANSHR', 'MEANALP'], axis=1)\n"
      ],
      "metadata": {
        "id": "I2tr2y7FLqia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cme_training_12['Timestamp'] = pd.to_datetime(df_cme_training_12['Timestamp']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "df_cme_training_12.tail()"
      ],
      "metadata": {
        "id": "RHF7FHBhLtC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cme_training_12 = df_cme_training_12.tail(df_cme_training_12.shape[0] -282)\n",
        "df_cme_training_12"
      ],
      "metadata": {
        "id": "DVzBeFZVLx4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get negative samples\n",
        "df_cme_training_12_neg = df_cme_training_12.loc[df_cme_training_12['Label'] == 'N']\n",
        "df_cme_training_12_neg = df_cme_training_12_neg.reset_index(drop=True)\n",
        "print('There are', df_cme_training_12_neg.shape[0], 'negative samples in the training set.\\n')\n",
        "df_cme_training_12_neg.tail()"
      ],
      "metadata": {
        "id": "zEr0N2bWL2D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get positive samples\n",
        "df_cme_training_12_pos = df_cme_training_12.loc[df_cme_training_12['Label'] == 'P']\n",
        "df_cme_training_12_pos = df_cme_training_12_pos.reset_index(drop = True)\n",
        "print('There are', df_cme_training_12_pos.shape[0], 'positive samples in the training set.\\n')\n",
        "df_cme_training_12_pos.tail()"
      ],
      "metadata": {
        "id": "U9DWKMCsL4uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing List"
      ],
      "metadata": {
        "id": "zEHj8IPoLVis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load list with both (neg, pos) labels of testing list for t= 12 h\n",
        "df_cme_test_12 = pd.read_csv(r'00_Dataset/event_lists/Liu2020_CME_data_samples/normalized_testing_12.csv', delimiter =',')\n",
        "df_cme_test_12.head()"
      ],
      "metadata": {
        "id": "lhg97FOiKSug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce dataframe to relevant columns\n",
        "df_cme_test_12 = df_cme_test_12.drop(['TOTUSJH',\t'TOTPOT',\t'TOTUSJZ',\t'ABSNJZH',\t'SAVNCPP',\n",
        "                     'USFLUX', 'AREA_ACR', 'MEANPOT', 'R_VALUE', 'SHRGT45',\n",
        "                     'MEANGAM', 'MEANJZH', 'MEANGBT', 'MEANGBZ', 'MEANJZD',\n",
        "                     'MEANGBH', 'MEANSHR', 'MEANALP'], axis=1)\n",
        "df_cme_test_12.head()"
      ],
      "metadata": {
        "id": "zuBvv2siLdUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert time stamp\n",
        "# convert time stamp such that sdo-cli can read them\n",
        "df_cme_test_12['Timestamp'] = pd.to_datetime(df_cme_test_12['Timestamp']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "df_cme_test_12.tail()"
      ],
      "metadata": {
        "id": "Aw7gSazyLido"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cme_test_12_neg = df_cme_test_12.loc[df_cme_test_12['Label'] == 'N']\n",
        "df_cme_test_12_neg = df_cme_test_12_neg.reset_index(drop=True)\n",
        "print('There are', df_cme_test_12_neg.shape[0], 'negative samples in the test set.\\n')\n",
        "df_cme_test_12_neg.tail()"
      ],
      "metadata": {
        "id": "Pw_Mqb8ALk2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cme_test_12_pos = df_cme_test_12.loc[df_cme_test_12['Label'] == 'P']\n",
        "df_cme_test_12_pos = df_cme_test_12_pos.reset_index(drop=True)\n",
        "print('There are', df_cme_test_12_pos.shape[0], 'positive samples in the test set.\\n')\n",
        "df_cme_test_12_pos.tail()"
      ],
      "metadata": {
        "id": "9TxMPF-eLnYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By now we have four event lists from the sampled dataset:\n",
        "\n",
        "\n",
        "*   df_cme_test_12_neg >> 762\n",
        "*   df_cme_test_12_pos >> 550\n",
        "*   df_cme_training_12_neg >> 16678\n",
        "*   df_cme_training_12_pos >> 3387 samples\n"
      ],
      "metadata": {
        "id": "YJGCym41L9PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Check the Image-Data"
      ],
      "metadata": {
        "id": "6Q0qk5BeIZpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training - negative"
      ],
      "metadata": {
        "id": "FXZZR881Iqfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create_sdo_aia_dataset(output_dir = './data/Liu2020_sampled/training_12/neg/', start_idx=0,  event_list = df_cme_training_12_neg, dt = '10min', wavelength = '171')"
      ],
      "metadata": {
        "id": "gPOAh0oXIwB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path_training_12_neg = '00_Dataset/Liu2020_sampled/training_12/neg/'\n",
        "result_training_12_neg, missing_files_training_neg = compare_filenames_with_dataframe(directory_path_training_12_neg, df_cme_training_12_neg, 171)"
      ],
      "metadata": {
        "id": "wvi6gwETJ3ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training - positive"
      ],
      "metadata": {
        "id": "JzsD_Y0WIwbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create_sdo_aia_dataset(output_dir = './data/Liu2020_sampled/training_12/pos/', start_idx=0, event_list = df_cme_training_12_pos, dt = '10min', wavelength = '171')"
      ],
      "metadata": {
        "id": "G-O8rxmUI00T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path_training_12_pos = '00_Dataset/Liu2020_sampled/training_12/pos/'\n",
        "result_training_12_pos, missing_files_training_pos = compare_filenames_with_dataframe(directory_path_training_12_pos, df_cme_training_12_pos, 171)"
      ],
      "metadata": {
        "id": "SRMNd2hQJ9l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing - negative"
      ],
      "metadata": {
        "id": "yIGXBNqYI1Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create_sdo_aia_dataset(output_dir = './data/Liu2020_sampled/test_12/neg/', start_idx=0, event_list = df_cme_test_12_neg, dt = '10min', wavelength = '171')"
      ],
      "metadata": {
        "id": "9BQhenuhI5HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path_test_12_neg = '00_Dataset/Liu2020_sampled/test_12/neg/'\n",
        "result_test_12_neg, missing_files_test_neg = compare_filenames_with_dataframe(directory_path_test_12_neg, df_cme_test_12_neg, 171)"
      ],
      "metadata": {
        "id": "T609Vl6ZKFS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing - positive"
      ],
      "metadata": {
        "id": "sQ3qHBwiI5w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create_sdo_aia_dataset(output_dir = './data/Liu2020_sampled/test_12/pos/', start_idx= 0, event_list = df_cme_test_12_pos, dt = '10min', wavelength = '171')"
      ],
      "metadata": {
        "id": "psygkyS9I9YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path_test_12_pos = '00_Dataset/Liu2020_sampled/test_12/pos/'\n",
        "result_test_12_neg, missing_test_pos = compare_filenames_with_dataframe(directory_path_test_12_pos, df_cme_test_12_pos, 171)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY1d4CInKKfX",
        "outputId": "c1f043a9-654b-40c2-a3a3-a4daddcdf12d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Directory Files:  550\n",
            "Total DataFrame Names:  550\n",
            "Common Files:  549\n",
            "Missing Files:  1\n",
            "Extra Files:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display a Sample Image-Series"
      ],
      "metadata": {
        "id": "pdeLVoxnI9s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display a sequence???\n",
        "\n"
      ],
      "metadata": {
        "id": "cE_aPMsWGgZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dvxPxrHuIhWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the ConvLSTM Model"
      ],
      "metadata": {
        "id": "7dGVeKZbGhfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    flare_label = sys.argv[1]\n",
        "    train_again = int(sys.argv[2])\n",
        "    filepath = './'\n",
        "    n_features = 0\n",
        "    if flare_label == 'C':\n",
        "        n_features = 14\n",
        "    start_feature = 5\n",
        "    mask_value = 0\n",
        "    series_len = 10\n",
        "    epochs = 7\n",
        "    batch_size = 256\n",
        "    nclass = 2\n",
        "    result_file = './output.csv'\n",
        "\n",
        "    if train_again == 1:\n",
        "        # Train\n",
        "        X_train_data, y_train_data = load_data(datafile=filepath + 'normalized_training.csv',\n",
        "                                               flare_label=flare_label, series_len=series_len,\n",
        "                                               start_feature=start_feature, n_features=n_features,\n",
        "                                               mask_value=mask_value)\n",
        "\n",
        "        X_train = np.array(X_train_data)\n",
        "        y_train = np.array(y_train_data)\n",
        "        y_train_tr = data_transform(y_train)\n",
        "\n",
        "        class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                          np.unique(y_train), y_train)\n",
        "        class_weight_ = {0: class_weights[0], 1: class_weights[1]}\n",
        "        # print(class_weight_)\n",
        "\n",
        "        model = lstm(nclass, n_features, series_len)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(X_train, y_train_tr,\n",
        "                            epochs=epochs, batch_size=batch_size,\n",
        "                            verbose=False, shuffle=True, class_weight=class_weight_)\n",
        "        model.save('./model.h5')\n",
        "    else:\n",
        "        model = load_model('./model.h5')\n",
        "\n",
        "    # Test\n",
        "    X_test_data, y_test_data = load_data(datafile=filepath + 'normalized_testing.csv',\n",
        "                                         flare_label=flare_label, series_len=series_len,\n",
        "                                         start_feature=start_feature, n_features=n_features,\n",
        "                                         mask_value=mask_value)\n",
        "    X_test = np.array(X_test_data)\n",
        "    y_test = np.array(y_test_data)\n",
        "    y_test_tr = data_transform(y_test)\n",
        "\n",
        "    classes = model.predict(X_test, batch_size=batch_size, verbose=0, steps=None)\n",
        "\n",
        "    with open(result_file, 'w', encoding='UTF-8') as result_csv:\n",
        "        w = csv.writer(result_csv)\n",
        "        with open(filepath + 'normalized_testing.csv', encoding='UTF-8') as data_csv:\n",
        "            reader = csv.reader(data_csv)\n",
        "            i = -1\n",
        "            for line in reader:\n",
        "                if i == -1:\n",
        "                    line.insert(0, 'Predicted Label')\n",
        "                else:\n",
        "                    if classes[i][0] >= 0.5:\n",
        "                        line.insert(0, 'Positive')\n",
        "                    else:\n",
        "                        line.insert(0, 'Negative')\n",
        "                i += 1\n",
        "                w.writerow(line)"
      ],
      "metadata": {
        "id": "-0zeELIlGh_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try: ChatGPT"
      ],
      "metadata": {
        "id": "aSR4TDqBGuNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, ConvLSTM2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Assuming you have already created the 'dataset' and 'labels' using the provided code\n",
        "\n",
        "# Normalize the pixel values in the dataset to a range [0, 1]\n",
        "dataset = dataset.astype('float32') / 255.0\n",
        "\n",
        "# Reshape the dataset to match the input shape expected by ConvLSTM\n",
        "# Assuming the images have shape (image_height, image_width, num_channels)\n",
        "# and 'sequence_length' images per sequence\n",
        "sequence_length, image_height, image_width, num_channels = dataset.shape\n",
        "input_shape = (sequence_length, image_height, image_width, num_channels)\n",
        "\n",
        "# Build the ConvLSTM model\n",
        "model = Sequential()\n",
        "model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_samples = int(0.8 * len(dataset))  # 80% for training\n",
        "x_train, y_train = dataset[:train_samples], labels[:train_samples]\n",
        "x_test, y_test = dataset[train_samples:], labels[train_samples:]\n",
        "\n",
        "# Train the ConvLSTM model\n",
        "epochs = 10  # Adjust the number of epochs based on your dataset and model complexity\n",
        "batch_size = 32\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "VmRWaNccoxOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model"
      ],
      "metadata": {
        "id": "EkhAmpJYGxv6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fe6X0-XQG0nP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}